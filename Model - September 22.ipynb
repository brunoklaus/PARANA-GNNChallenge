{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c09974",
   "metadata": {},
   "source": [
    "## Convert dataset to pytorch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e6c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('./dataset/gnnet-ch21-dataset-test/ch21-test-setting-1/100', 'results_100_400-2000_0_9.tar.gz')][('./dataset/gnnet-ch21-dataset-test/ch21-test-setting-1/95', 'results_95_400-2000_0_9.tar.gz')]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "from convertDataset import process_in_parallel, process_file\n",
    "process_in_parallel('test',4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd15c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch\n",
    "import re\n",
    "from torch_geometric.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ChallengeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Base class representing a dataset for the challenge.\n",
    "    \n",
    "    We assume that the conversion process is already done, i.e. we \n",
    "    work with a list of pytorch Data objects stored in .pt files,\n",
    "    all in the same folder.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def challenge_transform(self,data,converted_path=None,debug=False):\n",
    "        all_timeparams = ['EqLambda', 'AvgPktsLambda', 'ExpMaxFactor',\n",
    "                     'MinPktLambda','MaxPktLambda','StdDev','PktsLambdaOn',\n",
    "                      'AvgTOff','AvgTOn','BurstGenLambda','Bitrate',\n",
    "                      'ParetoMinSize','ParetoMaxSize','ParetoAlfa'\n",
    "                     ]\n",
    "\n",
    "        all_sizeparams = ['MinSize','MaxSize','AvgPktSize','PktSize1',\n",
    "                         'PktSize2','NumCandidates','Size_i','Prob_i']\n",
    "\n",
    "\n",
    "        \"\"\" 1a. Assert that some stuff remains constant...\"\"\"\n",
    "        if debug:\n",
    "            assert all(data.p_SizeDist == 2)\n",
    "            assert all(data.p_TimeDist == 0)\n",
    "            assert all(data.n_levelsQoS == 1)\n",
    "            assert all(data.p_ToS == 0.0)\n",
    "            assert all(data.n_queueSizes == 32)\n",
    "            assert data.n_schedulingPolicy[0] == \"FIFO\"\n",
    "            for a,v in zip(['p_size_AvgPktSize','p_size_PktSize1',\n",
    "                            'p_size_PktSize2', 'p_time_ExpMaxFactor'],\n",
    "                        [1000.0,300.0,1700.0,10.0]):\n",
    "                if not torch.allclose(getattr(data,a), \n",
    "                                      v*torch.ones_like(getattr(data,a)),rtol=1e-05):\n",
    "                    raise Exception(f\"{a} was expected to have the value close to {v}\")\n",
    "\n",
    "        del data.p_SizeDist, data.p_TimeDist, data.p_ToS\n",
    "        del data.n_queueSizes, data.n_levelsQoS, data.n_schedulingPolicy\n",
    "\n",
    "        \"\"\" 1b. Transform p_SizeDist and p_TimeDist into one-hot. We skip it because it \n",
    "        does not change in the training dataset.\n",
    "        \"\"\"\n",
    "        #data.p_SizeDist= (F.one_hot(data.p_SizeDist,4))\n",
    "        #data.p_TimeDist= (F.one_hot(data.p_SizeDist,6))\n",
    "\n",
    "        \"\"\" \n",
    "        2. Path attributes; Concatenate Size/Time distribution parameters.\n",
    "\n",
    "         It turns out all sizeparams have the same value. Useless.\n",
    "            Otherwise, we'd have:\n",
    "                data.p_sizeparams = torch.cat([getattr(data,a).view(-1,1) for a in sizeparams],axis=1)\n",
    "\n",
    "         Also, p_time_ExpMaxFactor is always equal to 10.0, so we delete it\n",
    "        \"\"\"\n",
    "        delattr(data,'p_time_ExpMaxFactor')\n",
    "\n",
    "        sizeparams = [f'p_size_{a}' for a in ['AvgPktSize','PktSize1',\n",
    "                     'PktSize2']]\n",
    "        timeparams = [f'p_time_{a}' for a in ['EqLambda', 'AvgPktsLambda']]\n",
    "    \n",
    "        p_params = timeparams + ['p_TotalPktsGen','p_PktsGen','p_AvgBw']\n",
    "        \n",
    "        mean_pkts_rate = data.p_time_AvgPktsLambda.mean().item()\n",
    "        \n",
    "        assert mean_pkts_rate > 0\n",
    "        for p in p_params:\n",
    "            setattr(data,p,getattr(data,p)/mean_pkts_rate)\n",
    "            \n",
    "        data.p_time_EqLambda /= 1000.0\n",
    "        data.p_AvgBw /= 1000.0\n",
    "        #data.p_time_EqLambda *= 0.0\n",
    "        data.p_TotalPktsGen *= 0.0\n",
    "        \n",
    "        \n",
    "        data.P = torch.cat([getattr(data,a).view(-1,1) for a in p_params],axis=1)\n",
    "        \n",
    "        \n",
    "        for p in p_params + sizeparams:\n",
    "            delattr(data,p)\n",
    "        \n",
    "        \"\"\"\n",
    "            3. Global Attributes\n",
    "        \"\"\"\n",
    "        global_attrs = ['g_delay','g_packets','g_losses','g_AvgPktsLambda']\n",
    "        data.G = torch.as_tensor([getattr(data,a) for a in global_attrs],device=data.P.device)\n",
    "        data.G = torch.tile(data.G.view(1,-1),(data.type.shape[0],1))\n",
    "        for a in global_attrs:\n",
    "            delattr(data,a)\n",
    "\n",
    "        \"\"\"\n",
    "            4. Link attributes\n",
    "        \"\"\"\n",
    "        data.L = data.l_capacity.clone().view(-1,1) / mean_pkts_rate\n",
    "        data.mean_pkts_rate = mean_pkts_rate*torch.ones_like(data.type)\n",
    "        \n",
    "        data.n_paths = data.P.shape[0]*torch.ones_like(data.type)\n",
    "        data.n_links = data.L.shape[0]*torch.ones_like(data.type)\n",
    "        \n",
    "        \n",
    "        delattr(data,'l_capacity')\n",
    "        \n",
    "        \n",
    "        if not converted_path is None:\n",
    "            torch.save(data,converted_path)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def __init__(self, root_dir,filenames=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the .pt files.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        if filenames is None:\n",
    "            onlyfiles = [f for f in os.listdir(self.root_dir) if osp.isfile(osp.join(self.root_dir, f))]\n",
    "            self.filenames = [f for f in onlyfiles if f.endswith('.pt')]\n",
    "        else:\n",
    "            self.filenames = filenames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        filename = self.filenames[idx]\n",
    "        pt_path = osp.join(self.root_dir, filename) \n",
    "        \n",
    "        converted_dir = self.root_dir+'_2'\n",
    "        #os.makedirs(converted_dir,exist_ok=True)\n",
    "        converted_path = osp.join(converted_dir, filename) \n",
    "        try:\n",
    "            sample = torch.load(pt_path,map_location='cuda')\n",
    "        except KeyboardInterrupt:\n",
    "            raise KeyboardInterrupt\n",
    "        except:\n",
    "            print(f\"Couldn't load {pt_path}\")\n",
    "        sample = self.challenge_transform(sample,converted_path=None)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f535a96b",
   "metadata": {},
   "source": [
    "\n",
    "## Divide validation datasets into 3. Initialize datasets/dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eeec346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_100_400-2000_0_9.tar.gz\n",
      "1    1040\n",
      "2    1040\n",
      "3    1040\n",
      "Name: validation_setting, dtype: int64\n",
      "results_100_400-2000_0_9.tar.gz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_path</th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>validation_setting</th>\n",
       "      <th>sample_num</th>\n",
       "      <th>file_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>test_42_0.pt</th>\n",
       "      <td>./dataset/gnnet-ch21-dataset-test/ch21-test-se...</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_42_1.pt</th>\n",
       "      <td>./dataset/gnnet-ch21-dataset-test/ch21-test-se...</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_42_2.pt</th>\n",
       "      <td>./dataset/gnnet-ch21-dataset-test/ch21-test-se...</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_42_3.pt</th>\n",
       "      <td>./dataset/gnnet-ch21-dataset-test/ch21-test-se...</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_42_4.pt</th>\n",
       "      <td>./dataset/gnnet-ch21-dataset-test/ch21-test-se...</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_61_5.pt</th>\n",
       "      <td>./dataset/gnnet-ch21-dataset-test/ch21-test-se...</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_61_6.pt</th>\n",
       "      <td>./dataset/gnnet-ch21-dataset-test/ch21-test-se...</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_61_7.pt</th>\n",
       "      <td>./dataset/gnnet-ch21-dataset-test/ch21-test-se...</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_61_8.pt</th>\n",
       "      <td>./dataset/gnnet-ch21-dataset-test/ch21-test-se...</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_61_9.pt</th>\n",
       "      <td>./dataset/gnnet-ch21-dataset-test/ch21-test-se...</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1560 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      full_path  num_nodes  \\\n",
       "test_42_0.pt  ./dataset/gnnet-ch21-dataset-test/ch21-test-se...         50   \n",
       "test_42_1.pt  ./dataset/gnnet-ch21-dataset-test/ch21-test-se...         50   \n",
       "test_42_2.pt  ./dataset/gnnet-ch21-dataset-test/ch21-test-se...         50   \n",
       "test_42_3.pt  ./dataset/gnnet-ch21-dataset-test/ch21-test-se...         50   \n",
       "test_42_4.pt  ./dataset/gnnet-ch21-dataset-test/ch21-test-se...         50   \n",
       "...                                                         ...        ...   \n",
       "test_61_5.pt  ./dataset/gnnet-ch21-dataset-test/ch21-test-se...        300   \n",
       "test_61_6.pt  ./dataset/gnnet-ch21-dataset-test/ch21-test-se...        300   \n",
       "test_61_7.pt  ./dataset/gnnet-ch21-dataset-test/ch21-test-se...        300   \n",
       "test_61_8.pt  ./dataset/gnnet-ch21-dataset-test/ch21-test-se...        300   \n",
       "test_61_9.pt  ./dataset/gnnet-ch21-dataset-test/ch21-test-se...        300   \n",
       "\n",
       "              validation_setting  sample_num  file_num  \n",
       "test_42_0.pt                   1           0        42  \n",
       "test_42_1.pt                   1           1        42  \n",
       "test_42_2.pt                   1           2        42  \n",
       "test_42_3.pt                   1           3        42  \n",
       "test_42_4.pt                   1           4        42  \n",
       "...                          ...         ...       ...  \n",
       "test_61_5.pt                   3           5        61  \n",
       "test_61_6.pt                   3           6        61  \n",
       "test_61_7.pt                   3           7        61  \n",
       "test_61_8.pt                   3           8        61  \n",
       "test_61_9.pt                   3           9        61  \n",
       "\n",
       "[1560 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_val =  ChallengeDataset(root_dir='./dataset/converted_validation')\n",
    "ds_test =  ChallengeDataset(root_dir='./dataset/converted_test')\n",
    "\n",
    "filenames_val = ds_val.filenames\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datanetAPI\n",
    "import os.path as osp\n",
    "def converted_filenames_metadata(filenames,path_to_original_dataset):\n",
    "        import re\n",
    "        \n",
    "        def m(f):\n",
    "            g = re.match(\"(validation|train|test)\\_(\\d+)\\_(\\d+).*\",f).groups()\n",
    "            g  = [g[0], int(g[1]), int(g[2])]\n",
    "            return g\n",
    "        \n",
    "        matches =  [m(f) for f in filenames]\n",
    "        reader = datanetAPI.DatanetAPI(path_to_original_dataset)\n",
    "        files_num = np.array([m[1] for m in matches],dtype=np.int32)\n",
    "        samples_num = np.array([m[2] for m in matches],dtype=np.int32)\n",
    "        \n",
    "        all_paths = np.array(reader.get_available_files())\n",
    "        print(all_paths[0,1])\n",
    "        df = pd.DataFrame(index=filenames,columns=['full_path','num_nodes','validation_setting'])\n",
    "        df['full_path'] = all_paths[files_num,0]\n",
    "        df['sample_num'] = samples_num\n",
    "        df['file_num'] = files_num\n",
    "        \n",
    "        df['num_nodes'] = np.array([osp.split(f)[-1] for f in df['full_path'].values],dtype=np.int32)\n",
    "        \n",
    "        if matches[0][0] in ['validation','test']:\n",
    "            df['validation_setting'] = np.array([osp.split(f)[-2][-1] for f in df['full_path'].values],dtype=np.int32)\n",
    "        else:\n",
    "            df['validation_setting'] = -1\n",
    "            \n",
    "        \"\"\"\n",
    "            Put it in correct order\n",
    "        \"\"\"\n",
    "        df = df.sort_values(by=['validation_setting','num_nodes','file_num','sample_num'])\n",
    "        return df\n",
    "        \n",
    "        \n",
    "df_val = converted_filenames_metadata(ds_val.filenames,f'./dataset/gnnet-ch21-dataset-{\"validation\"}')\n",
    "\"\"\"\n",
    "    We opt to make the validation set smaller (as it is more time consuming to run)\n",
    "\"\"\"\n",
    "print(df_val['validation_setting'].value_counts())\n",
    "df_val['filenames'] = df_val.index.values\n",
    "df_val = df_val.groupby('full_path').head(10)\n",
    "df_test =  converted_filenames_metadata(ds_test.filenames,f'./dataset/gnnet-ch21-dataset-{\"test\"}')\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c30c4e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Len (train): 120000\n",
      "Dataloader Len (train): 7500\n",
      "Dataset Len (val): 3120\n",
      "Dataloader Len (val): 780\n",
      "Dataset Len (test): 1560\n",
      "Dataloader Len (test): 390\n",
      "Dataset Len (val_1): 260\n",
      "Dataloader Len (val_1): 65\n",
      "Dataset Len (val_2): 260\n",
      "Dataloader Len (val_2): 65\n",
      "Dataset Len (val_3): 260\n",
      "Dataloader Len (val_3): 65\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = {'train':16,'val':4}\n",
    "datasets = {\"train\": ChallengeDataset(root_dir='/usr/converted_train/converted_train'),\n",
    "            \"val\":ChallengeDataset(root_dir='./dataset/converted_validation'),\n",
    "            \"test\":ChallengeDataset(root_dir='./dataset/converted_test',\n",
    "                                   filenames=list(df_test.index) )\n",
    "           }\n",
    "for i in range(3):\n",
    "    which_files = list(df_val[df_val['validation_setting']==i+1]['filenames'].values)\n",
    "    ds = ChallengeDataset(root_dir='./dataset/converted_validation',\n",
    "                         filenames=which_files)\n",
    "    datasets[f'val_{i+1}'] = ds\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "dataloaders = {}\n",
    "for k in datasets.keys():\n",
    "    if k.startswith('train'):\n",
    "        batch_size = BATCH_SIZE['train']\n",
    "    else:\n",
    "        batch_size = BATCH_SIZE['val']\n",
    "    dataloaders[k] = DataLoader(datasets[k],batch_size=batch_size,shuffle=False)\n",
    "    dataloaders[k+\"_s\"] = DataLoader(datasets[k],batch_size=batch_size,shuffle=True)\n",
    "    print(f\"Dataset Len ({k}): {len(datasets[k])}\")\n",
    "    print(f\"Dataloader Len ({k}): {len(dataloaders[k])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146e0c01",
   "metadata": {},
   "source": [
    "## (Debug) Optionally, run this to ensure that the training, validation and test datasets are correctly loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "9d058ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 259/1560 [00:02<00:14, 87.76it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-15dfa748da3c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gcn2/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gcn2/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mbyte\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-250-6ba3a926d8c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-15dfa748da3c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't load {pt_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "DEBUG_MODES  = ['train','val','test']\n",
    "for mode in DEBUG_MODES:\n",
    "    for i in tqdm(range(len(datasets[mode]))):\n",
    "        X = datasets[mode][i]\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "327c5b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(G=[817852, 4], L=[90852, 1], P=[708410, 5], batch=[817852], edge_index=[2, 3322522], edge_pos=[3322522], edge_pos_rel=[3322522], edge_type=[3322522], mean_pkts_rate=[817852], n_links=[817852], n_paths=[817852], out_delay=[708410], out_occupancy=[90852], ptr=[501], type=[817852])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from scipy.stats._continuous_distns import _distn_names\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (16.0, 12.0)\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "def get_train_distribution_statistics(ds_train,attrs_to_normalize,calc_distr=False):\n",
    "    torch.manual_seed(42)\n",
    "    dl_train_oneshot = DataLoader(ds_train,batch_size=500,shuffle=True)\n",
    "    for sample in dl_train_oneshot:\n",
    "        print(sample)\n",
    "        means=dict([(k, torch.mean(getattr(sample,k).float(),axis=0))\\\n",
    "                   for k in attrs_to_normalize])\n",
    "        std=dict([(k, torch.std(getattr(sample,k).float(),axis=0))\\\n",
    "                   for k in attrs_to_normalize])\n",
    "        sample.out_occupancy.cpu().numpy()\n",
    "        distr = None\n",
    "        if calc_distr:\n",
    "            distr =  (best_fit_distribution(sample.out_occupancy.cpu().numpy()))\n",
    "            plt.show()\n",
    "            plt.hist(sample.out_occupancy.cpu().numpy()*100.0,bins=100)\n",
    "            plt.show()\n",
    "        break\n",
    "    return means, std, distr\n",
    "\n",
    "means, stds, _ = get_train_distribution_statistics(datasets['train'],['P','G','L','out_occupancy','out_delay'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ede780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8259a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU, Sigmoid\n",
    "from torch_geometric.nn.conv import GATConv,TransformerConv,GCNConv,GINConv\n",
    "\n",
    "from torch_geometric_temporal.nn.recurrent import GConvGRU\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter\n",
    "\n",
    "def separate_edge_timesteps(edge_index,edge_type):\n",
    "    all_edges= [[] for _ in range(3)]\n",
    "    for et in [0,1,2]:\n",
    "        et_edges = edge_index[:,edge_type==et]\n",
    "\n",
    "        init_tensor = torch.cat([torch.ones(1,device=et_edges.device).long(),torch.diff(et_edges[0,:])],dim=0)\n",
    "        init_tensor = torch.clip(torch.abs(init_tensor),0.,1.)\n",
    "        # [1, 0, 0, 0, 1, 0, 0, 1] where 0 iff edge source equal\n",
    "        \"\"\"  Debug: \n",
    "        init_tensor = torch.as_tensor([1,0, 0, 0, 1, 0, 0, 1] ) \n",
    "        sol =  [0,1,2,3,0,1,2,0]\n",
    "        \"\"\"\n",
    "        # [0, 1, 1, 1, 0, 1, 1, 0] where 1 iff edge source equal\n",
    "        init_tensor = 1 - init_tensor\n",
    "        # [0, 1, 1, 1, -4, 1, 1, -3] where 1 iff edge source equal\n",
    "        count_tensor = torch.nonzero(1-init_tensor).view(-1)\n",
    "        init_tensor[count_tensor[1:]] = -torch.diff(count_tensor) +1\n",
    "        # [0, 1, 2, 3, 0, 1, 2, 0] where 1 iff edge source equal\n",
    "        init_tensor = init_tensor.cumsum(axis=0)\n",
    "\n",
    "        # Will list all dsts that were the first linked to some src, then all second, etc..\n",
    "        ensure_stable = torch.linspace(start=0.0,end=0.5,steps=init_tensor.shape[0],device=init_tensor.device)\n",
    "        encountered_order = torch.sort(init_tensor+ensure_stable)[1]\n",
    "        et_edges = et_edges[:,encountered_order]\n",
    "        \n",
    "        \n",
    "        #vals[i] == number of edges that belong to time step i\n",
    "        idxs, vals = torch.unique(init_tensor,return_counts=True)\n",
    "        vs = [x for x in torch.split_with_sizes(et_edges,tuple(vals),dim=1)]\n",
    "         \n",
    "        \n",
    "        #if not torch.as_tensor([v.shape[1] for v in vs]).sum().item() == et_edges.shape[1]:\n",
    "        #    raise f\"Sum of disjoint timesteps is {torch.as_tensor([v.shape[0] for v in vs]).sum()} but should be {et_edges.shape[1]}\"\n",
    "\n",
    "        all_edges[et] = vs\n",
    "        \n",
    "    return all_edges\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "\n",
    "class Baseline(torch.nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self,num_iterations=5,G_dim = 4,P_dim =5,L_dim = 1,**kwargs):\n",
    "        super(Baseline, self).__init__(**kwargs)\n",
    "        self.num_iterations = num_iterations\n",
    "        self.G_dim = G_dim\n",
    "        self.P_dim = P_dim\n",
    "        self.L_dim = L_dim\n",
    "        \n",
    "        self.H = 2\n",
    "        self.H_p = 2\n",
    "        self.H_l = 2\n",
    "        self.H_n = 2\n",
    "        \n",
    "        self.conv_pn_1 = []\n",
    "        self.conv_ln_1 = []\n",
    "        self.conv_pn_r_1 = []\n",
    "        self.conv_ln_r_1 = []\n",
    "        self.conv_pl_1 = []\n",
    "        self.conv_pl_r_1 = []\n",
    "        \n",
    "        for i in range(self.num_iterations):\n",
    "            self.conv_pn_1.append(GCNConv(self.H_p+self.P_dim,self.H_n).cuda())\n",
    "            self.conv_ln_1.append(GCNConv(self.H_l+L_dim,self.H_n).cuda())\n",
    "            self.conv_pl_1.append(GCNConv(self.H_p+P_dim,self.H_l).cuda())\n",
    "\n",
    "            self.conv_pn_r_1.append(GCNConv(self.H_n+G_dim,self.H_p,flow=\"target_to_source\").cuda())\n",
    "            self.conv_ln_r_1 .append(GCNConv(self.H_n+G_dim,self.H_l,flow=\"target_to_source\").cuda())\n",
    "        \n",
    "            self.conv_pl_r_1.append(GConvGRU(self.H_l+L_dim,self.H_p,K=2).cuda())\n",
    "        \n",
    "        self.finalconv = GCNConv(self.H_p+P_dim,self.H_l,normalize=True).cuda()\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(L_dim+self.H_l,512)\n",
    "        #self.lin2 = torch.nn.Linear(512,512)\n",
    "        self.lin3 = torch.nn.Linear(512,1)\n",
    "        self.xlin1 = torch.nn.Linear(self.H_n+self.G_dim+self.H_p+self.P_dim+self.H_l+self.L_dim,128)\n",
    "        self.xlin2 = torch.nn.Linear(128,self.H_n+self.G_dim+self.H_p+self.P_dim+self.H_l+self.L_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self,data,means,stds,naive=True,mode=None):\n",
    "        edge_index = data.edge_index.long()\n",
    "        edge_type = data.edge_type.clone()\n",
    "        \n",
    "        is_p = data.type==0\n",
    "        is_l = data.type==1\n",
    "        is_n = data.type==2       \n",
    "        \n",
    "        \n",
    "        all_edges = separate_edge_timesteps(edge_index,edge_type)\n",
    "        \n",
    "        \"\"\" Each element $i$ of pl_by_time contains all edges that occur at  \n",
    "            position $i$ in some path.\n",
    "        \"\"\"\n",
    "        pl_at_time = all_edges[0]\n",
    "        \n",
    "        edges_pl = edge_index[:,edge_type==0]\n",
    "        edges_pn = edge_index[:,edge_type==1]\n",
    "        edges_ln = edge_index[:,edge_type==2]    \n",
    "        \n",
    "\n",
    "        G_dim, P_dim, L_dim = self.G_dim, self.P_dim, self.L_dim\n",
    "        H_n, H_p, H_l = self.H_n, self.H_p, self.H_l\n",
    "        \n",
    "        n_p = torch.sum(is_p)\n",
    "        n_l = torch.sum(is_l)\n",
    "        n_n = torch.sum(is_n)\n",
    "        \n",
    "        \"\"\" Get true value of P,L,G\"\"\"\n",
    "        P = data.P * data.mean_pkts_rate[is_p].view(-1,1)\n",
    "        L = data.L * data.mean_pkts_rate[is_l].view(-1,1)\n",
    "        L = L / 1000\n",
    "        G = data.G \n",
    "        \n",
    "          \n",
    "        \n",
    "        cnt = 0\n",
    "        cnt, node_hidden = cnt+H_n,   slice(cnt,cnt+H_n)\n",
    "        cnt, node_og     = cnt+G_dim, slice(cnt,cnt+G_dim)\n",
    "        cnt, link_hidden = cnt+H_l,   slice(cnt,cnt+H_l)\n",
    "        cnt, link_og     = cnt+L_dim, slice(cnt,cnt+L_dim)\n",
    "        cnt, path_hidden = cnt+H_p,   slice(cnt,cnt+H_p)\n",
    "        cnt, path_og     = cnt+P_dim, slice(cnt,cnt+P_dim)\n",
    "        \n",
    "        cnt = 0\n",
    "        cnt, node_all = cnt+H_n+G_dim, slice(cnt,cnt+H_n+G_dim)\n",
    "        cnt, link_all = cnt+H_l+L_dim, slice(cnt,cnt+H_l+L_dim)\n",
    "        cnt, path_all = cnt+H_p+P_dim, slice(cnt,cnt+H_p+P_dim)\n",
    "        \n",
    "        X = torch.zeros(data.G.size(0),H_n+G_dim+H_p+P_dim+H_l+L_dim,device='cuda')\n",
    "        X[:,node_og] = G[:,:]\n",
    "        X[is_l,link_og] = L\n",
    "        X[is_p,path_og] = P\n",
    "        \n",
    "        x1 = torch.clip(F.leaky_relu(self.xlin1(X)),0,0)\n",
    "        \n",
    "        \"\"\"\n",
    "            Get Average bandwidth\n",
    "        \"\"\"\n",
    "        A = X[:,path_og.stop-2].view(-1,).clone() #Avg pkts sent\n",
    "        blocking_probs =  0.3*torch.ones_like(A)\n",
    "        \n",
    "        max_numpaths = len(pl_at_time) \n",
    "        T =  torch.zeros(X.size(0),device=A.device)\n",
    "        rhos =  torch.zeros(X.size(0),device=A.device)      \n",
    "        \n",
    "        \"\"\"\n",
    "            \\trafic[k]_{i}: traffic passing on some edge that appears in order k at path\n",
    "        \"\"\"\n",
    "        def update_traffic(L,T,A,pl_at_time,blocking_probs):\n",
    "            multiplier = 1.0\n",
    "            T = torch.zeros_like(T)\n",
    "            N =  torch.zeros_like(T)\n",
    "            for k in range(max_numpaths):\n",
    "                \n",
    "                if k == 0:\n",
    "                    \"\"\" Just map the demand on the respective path\"\"\"\n",
    "                    traffic = A.clone()\n",
    "                else:\n",
    "                    prev_paths = pl_at_time[k-1][0,:]\n",
    "                    prev_edges = pl_at_time[k-1][1,:]\n",
    "                    prev_edges_block_probs = torch.gather(blocking_probs,dim=0,\n",
    "                                                         index=prev_edges)\n",
    "\n",
    "                    traffic[prev_paths] *= (1.0 - prev_edges_block_probs)\n",
    "                    \n",
    "                which_paths = pl_at_time[k][0,:]\n",
    "                which_edges = pl_at_time[k][1,:]\n",
    "                T += scatter(src=torch.gather(traffic,0,which_paths),\n",
    "                             index=which_edges,\n",
    "                            dim=0,dim_size=X.size(0),reduce='sum')\n",
    "                N += scatter(src=torch.ones_like(torch.gather(traffic,0,which_paths)),\n",
    "                             index=which_edges,\n",
    "                            dim=0,dim_size=X.size(0),reduce='sum')\n",
    "                #print(T[is_l].mean())    \n",
    "            #T = T/torch.maximum(N,torch.ones_like(N))\n",
    "            #T /= max_numpaths\n",
    "            return T,N\n",
    "        B = buffer_size = 32\n",
    "        def update_blocking_probs(L,T,A,pl_at_time,blocking_probs):\n",
    "            blocking_probs = 0.0*blocking_probs\n",
    "            rhos = 0.0*blocking_probs      \n",
    "            rhos[is_l] = T[is_l] / X[is_l,link_og.start]\n",
    "            #print(rhos[is_l].mean())\n",
    "            \n",
    "            blocking_probs_num = (1.0 - rhos) * torch.pow(rhos,buffer_size)\n",
    "            blocking_probs_den = 1.0 - torch.pow(rhos,buffer_size+1)\n",
    "            return blocking_probs_num/(blocking_probs_den+1e-08)\n",
    "        \n",
    "        for t in range(self.num_iterations):\n",
    "            T, N = update_traffic(L,T,A,pl_at_time,blocking_probs)\n",
    "            #print(\"mean traffic: \",T[is_l].mean().item())\n",
    "            \n",
    "            blocking_probs = update_blocking_probs(L,T,A,pl_at_time,blocking_probs)\n",
    "\n",
    "            #print(\"mean block p.: \",blocking_probs[is_l].mean().item())\n",
    "            rhos = T[is_l] / (X[is_l,link_og.start])\n",
    "            pi_0 = (1 - rhos)/(1-torch.pow(rhos,B+1))\n",
    "            res = 1*pi_0\n",
    "            for j in range(32):\n",
    "                pi_0 = pi_0*rhos\n",
    "                res += (j+1)*pi_0\n",
    "                \n",
    "            res = res/32\n",
    "            #res_n1 =  rhos * (1+buffer_size)*torch.pow(rhos,B+1)\n",
    "            #res_d1 = 1 - torch.pow(rhos,B+1)\n",
    "            #res = (rhos) / (1-rhos)\n",
    "            #res =  res-res_n1/res_d1\n",
    "            #res = res/32\n",
    "            #res += 1/32\n",
    "            \n",
    "        #res += data.out_occupancy.min()\n",
    "        \"\"\"\n",
    "            Get avg occupancy\n",
    "        \"\"\"\n",
    "        if False:\n",
    "            import matplotlib.pyplot as plt\n",
    "            print(data.out_occupancy.min())\n",
    "            print(data.out_occupancy.mean())\n",
    "            print('rhos range',rhos.clone().min(),rhos.clone().max())\n",
    "            print('res range',res.clone().min(),res.clone().max())\n",
    "            print('data occup range',data.out_occupancy.min(),\n",
    "                  data.out_occupancy.clone().max())\n",
    "            print(\"1/32 = \",1/32)\n",
    "            print(rhos.min())\n",
    "            print(T[is_l].min())\n",
    "\n",
    "            plt.title(\"Occupancy\")\n",
    "            plt.hist(data.out_occupancy.clone().detach().cpu().numpy(),bins=100,color='green',alpha=0.5)\n",
    "            plt.hist(res.clone().detach().cpu().numpy(),bins=100)\n",
    "            plt.axvline(res.mean().item(),c='purple')\n",
    "            plt.show()\n",
    "            plt.title(\"Traffic\")\n",
    "            plt.hist(T[is_l].clone().detach().cpu().numpy(),bins=100)\n",
    "            plt.axvline(T[is_l].mean().item(),c='purple')\n",
    "            plt.show()\n",
    "            plt.title(\"N\")\n",
    "            plt.hist(N[is_l].clone().detach().cpu().numpy(),bins=100)\n",
    "            plt.axvline(N[is_l].mean().item(),c='purple')\n",
    "            plt.show()\n",
    "\n",
    "            print(res.mean())\n",
    "            raise ''\n",
    "        L = res + x1[is_l,0]\n",
    "        \n",
    "        \"\"\" To predict the node, we use the formula:\n",
    "        \n",
    "            path delay ~= \\sum_{i=0}^{n_links} delay_link(i)\n",
    "            where \n",
    "                delay_link(i) := avg_utilization_{i} * (queue_size_{i}/link_capacity_{i})\n",
    "                \n",
    "            Our NN predicts avg_utilization_{i} \\in [0,1], \\forall i. \n",
    "            For this dataset, we have \\forall i: queue_size_{i} =32000\n",
    "        \"\"\"\n",
    "        X = torch.zeros(X.size(0), device=X.device)\n",
    "        data_L = data.L.squeeze(-1)\n",
    "        link_capacity = data.L.squeeze(-1) * data.mean_pkts_rate[is_l]\n",
    "        X[is_l] = L.squeeze(-1)  * 32000.0  / link_capacity\n",
    "        E   = torch.gather(X,index=edges_pl[1,:],dim=0)\n",
    "        res = scatter(src=E,index=edges_pl[0,:],dim=0,dim_size=X.size(0),reduce='sum')\n",
    "        res = res[is_p]\n",
    "        return res, L\n",
    "        \n",
    "        \n",
    "\n",
    "class ChallengeModel(torch.nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self,num_iterations=3,G_dim = 4,P_dim =5,L_dim = 1,**kwargs):\n",
    "        super(ChallengeModel, self).__init__(**kwargs)\n",
    "        self.num_iterations = num_iterations\n",
    "        self.G_dim = G_dim\n",
    "        self.P_dim = P_dim\n",
    "        self.L_dim = L_dim\n",
    "        \n",
    "        self.H = 64\n",
    "        self.H_p = 64\n",
    "        self.H_l = 64\n",
    "        self.H_n = 64\n",
    "        \n",
    "        self.conv_pn_1 = []\n",
    "        self.conv_ln_1 = []\n",
    "        self.conv_pn_r_1 = []\n",
    "        self.conv_ln_r_1 = []\n",
    "        self.conv_pl_1 = []\n",
    "        self.conv_pl_r_1 = []\n",
    "        \n",
    "        for i in range(self.num_iterations):\n",
    "            self.conv_pn_1.append(GATConv(self.H_p+self.P_dim,self.H_n).cuda())\n",
    "            self.conv_ln_1.append(GATConv(self.H_l+L_dim,self.H_n).cuda())\n",
    "            self.conv_pl_1.append(GATConv(self.H_p+P_dim,self.H_l).cuda())\n",
    "\n",
    "            self.conv_pn_r_1.append(GATConv(self.H_n+G_dim,self.H_p,flow=\"target_to_source\").cuda())\n",
    "            self.conv_ln_r_1 .append(GATConv(self.H_n+G_dim,self.H_l,flow=\"target_to_source\").cuda())\n",
    "        \n",
    "            self.conv_pl_r_1.append(GConvGRU(self.H_l+L_dim,self.H_p,K=2).cuda())\n",
    "        \n",
    "        self.finalconv = GCNConv(self.H_p+P_dim,self.H_l,normalize=True).cuda()\n",
    "        \n",
    "        \n",
    "        for c in ['conv_pn','conv_ln','conv_pl']:\n",
    "            setattr(self,c+'_1',torch.nn.ModuleList(getattr(self,c+'_1')))\n",
    "            setattr(self,c+'_r_1',torch.nn.ModuleList(getattr(self,c+'_r_1')))\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(L_dim+self.H_l,512)\n",
    "        self.lin2 = torch.nn.Linear(512,512)\n",
    "        self.lin3 = torch.nn.Linear(512,1)\n",
    "        self.xlin1 = torch.nn.Linear(self.H_n+self.G_dim+self.H_p+self.P_dim+self.H_l+self.L_dim,128)\n",
    "        self.xlin2 = torch.nn.Linear(128,self.H_n+self.G_dim+self.H_p+self.P_dim+self.H_l+self.L_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self,data,means,stds,naive=True,mode=None,baseline_occup=None,baseline_out=None):\n",
    "        edge_index = data.edge_index.long()\n",
    "        edge_type = data.edge_type.clone()\n",
    "        \n",
    "        is_p = data.type==0\n",
    "        is_l = data.type==1\n",
    "        is_n = data.type==2       \n",
    "        \n",
    "        \n",
    "        all_edges = separate_edge_timesteps(edge_index,edge_type)\n",
    "        vs = all_edges[0]\n",
    "        \n",
    "        edges_pl = edge_index[:,edge_type==0]\n",
    "        edges_pn = edge_index[:,edge_type==1]\n",
    "        edges_ln = edge_index[:,edge_type==2]    \n",
    "        \n",
    "        #print(f\"Edges between path and link: {edges_pl.shape[1]}\")\n",
    "        #print(f\"Edges between path and node: {edges_pn.shape[1]}\")\n",
    "        #print(f\"Edges between link and node: {edges_ln.shape[1]}\")\n",
    "    \n",
    "        \n",
    "        G_dim, P_dim, L_dim = self.G_dim, self.P_dim, self.L_dim\n",
    "        H_n, H_p, H_l = self.H_n, self.H_p, self.H_l\n",
    "        \n",
    "        n_p = torch.sum(is_p)\n",
    "        n_l = torch.sum(is_l)\n",
    "        n_n = torch.sum(is_n)\n",
    "        \n",
    "        P = (data.P - means['P'])/(1e-08+stds['P'])\n",
    "        #P =  data.P\n",
    "        G = 0.0*(data.G - means['G'])/stds['G']\n",
    "        G[:,2] = 0*data.G[:,2]\n",
    "        L = (data.L - means['L'])/stds['L']\n",
    "        \n",
    "        \n",
    "        \n",
    "        maxval = (100000 - means['L'])/stds['L']\n",
    "        #L = torch.min(maxval*torch.ones_like(L),L)\n",
    "        #print(f\"Paths ({n_p}) Links ({n_l}) Nodes({n_n})\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        X = torch.zeros(data.G.size(0),H_n+G_dim+H_p+P_dim+H_l+L_dim,device='cuda')\n",
    "        cnt = 0\n",
    "        cnt, node_hidden = cnt+H_n,   slice(cnt,cnt+H_n)\n",
    "        cnt, node_og     = cnt+G_dim, slice(cnt,cnt+G_dim)\n",
    "        cnt, link_hidden = cnt+H_l,   slice(cnt,cnt+H_l)\n",
    "        cnt, link_og     = cnt+L_dim, slice(cnt,cnt+L_dim)\n",
    "        cnt, path_hidden = cnt+H_p,   slice(cnt,cnt+H_p)\n",
    "        cnt, path_og     = cnt+P_dim, slice(cnt,cnt+P_dim)\n",
    "        \n",
    "        cnt = 0\n",
    "        cnt, node_all = cnt+H_n+G_dim, slice(cnt,cnt+H_n+G_dim)\n",
    "        cnt, link_all = cnt+H_l+L_dim, slice(cnt,cnt+H_l+L_dim)\n",
    "        cnt, path_all = cnt+H_p+P_dim, slice(cnt,cnt+H_p+P_dim)\n",
    "        \n",
    "        \n",
    "        X[:,node_og] = G[:,:]\n",
    "        #print(X)\n",
    "        #print(L)\n",
    "        X[is_l,link_og] = L\n",
    "        X[is_p,path_og] = P\n",
    "        \n",
    "        X[is_l,link_hidden.start] = baseline_occup\n",
    "        X[is_p,path_hidden.start] = baseline_out\n",
    "        \n",
    "        \n",
    "        X = F.leaky_relu(self.xlin1(X))\n",
    "        X = F.leaky_relu(self.xlin2(X))\n",
    "        \n",
    "        #print(X[is_n,node_hidden].shape)\n",
    "        #print(X[is_l,link_hidden].shape)\n",
    "        #print(X[is_p,path_hidden].shape)\n",
    "        def act(x):\n",
    "            return F.leaky_relu(x)\n",
    "        for i in range(self.num_iterations):\n",
    "            X[is_p,path_hidden] =  act(self.conv_pn_r_1[i](X[:,node_all].clone(),edges_pn)[is_p,:])\n",
    "            \n",
    "            x = X[:,link_all].clone()\n",
    "            H = None\n",
    "            max_numpaths = len(vs)\n",
    "            for k in range(max_numpaths):\n",
    "                e = torch.cat([vs[k][1,:].unsqueeze(0),vs[k][0,:].unsqueeze(0)],axis=0)\n",
    "                H  = self.conv_pl_r_1[0](X=x,H=H,edge_index=e)    \n",
    "            X[is_p,path_hidden] = act(H[is_p,:]/max_numpaths)\n",
    "            X[is_p,path_hidden.start] = baseline_out\n",
    "            \n",
    "            X[is_n,node_hidden] = \\\n",
    "                act(self.conv_pn_1[i](X[:,path_all].clone(), edges_pn)[is_n,:] +\\\n",
    "                              self.conv_ln_1[i](X[:,link_all].clone(),edges_ln)[is_n,:])\n",
    "\n",
    "            \n",
    "            X[is_l,link_hidden] =  act(self.conv_ln_r_1[i](X[:,node_all].clone(),edges_ln)[is_l,:])\n",
    "            X[is_l,link_hidden] =  act(self.conv_pl_1[i](X[:,path_all].clone(),\n",
    "                                                         edges_pl)[is_l,:])\n",
    "            X[is_l,link_hidden.start] = baseline_occup\n",
    "        \n",
    "        #X[is_l,link_hidden] =  self.finalconv(X[:,path_all].clone(),edges_pl)[is_l,:] \n",
    "            \n",
    "        L = X[is_l,link_all]\n",
    "        L = self.lin1(L)\n",
    "        \n",
    "        L = F.leaky_relu(L)\n",
    "        L = F.leaky_relu(self.lin2(L))\n",
    "        #L = F.leaky_relu(L)\n",
    "        L = torch.sigmoid(self.lin3(L)) \n",
    "        #lamb = (1/0.05)\n",
    "        #L = -(1/lamb)* torch.log(1-0.99*L)\n",
    "        \n",
    "        \n",
    "        X = torch.zeros(X.size(0), device=X.device)\n",
    "        \n",
    "        \n",
    "        \"\"\" To predict the node, we use the formula:\n",
    "        \n",
    "            path delay ~= \\sum_{i=0}^{n_links} delay_link(i)\n",
    "            where \n",
    "                delay_link(i) := avg_utilization_{i} * (queue_size_{i}/link_capacity_{i})\n",
    "                \n",
    "            Our NN predicts avg_utilization_{i} \\in [0,1], \\forall i. \n",
    "            For this dataset, we have \\forall i: queue_size_{i} =32000\n",
    "        \"\"\"\n",
    "        \n",
    "        maxval = 100000\n",
    "        data_L = data.L.squeeze(-1)\n",
    "        multiplier = torch.minimum(torch.ones_like(data_L),100000/data_L)\n",
    "        link_capacity = data.L.squeeze(-1) * data.mean_pkts_rate[is_l]\n",
    "        \n",
    "        \n",
    "        \n",
    "        X[is_l] = L.squeeze(-1)  * 32000.0  / link_capacity\n",
    "        E   = torch.gather(X,index=edges_pl[1,:],dim=0)\n",
    "        \n",
    "        #print(f\"Shape after gather {E.shape}\")\n",
    "        res = scatter(src=E,index=edges_pl[0,:],dim=0,dim_size=X.size(0),reduce='sum')\n",
    "        res = res[is_p]\n",
    "        \n",
    "        #print(f\"Final per-path-mean delay shape:{res.shape}\")\n",
    "        #if naive:\n",
    "        #    return sample.G[sample.batch[is_p],0] + 0*res\n",
    "        return res, L\n",
    "               \n",
    "        \n",
    "       \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f70cca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:02<00:00,  3.98it/s]\n",
      "  2%|▏         | 1/65 [00:00<00:11,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 0 - Avg stats (train)\n",
      "Mean loss: 87.9139259185791\n",
      "Mean out: 0.1944429641366005\n",
      "Mean occup: 0.08387290467321873\n",
      "Mean actual_out: 0.14265514215826988\n",
      "Mean actual_occup: 0.05484498584270477\n",
      "tensor([[0.1222, 0.1163],\n",
      "        [0.1224, 0.1196],\n",
      "        [0.1975, 0.1938],\n",
      "        ...,\n",
      "        [0.1493, 0.1463],\n",
      "        [0.1042, 0.0999],\n",
      "        [0.3394, 0.3315]], device='cuda:0')\n",
      "tensor(4.1473, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:13<00:00,  4.67it/s]\n",
      "  2%|▏         | 1/65 [00:00<00:07,  8.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 0 - Avg stats (val_1)\n",
      "Mean loss: 13.921687602996826\n",
      "Mean out: 0.5440470058184403\n",
      "Mean occup: 0.03990634582363642\n",
      "Mean actual_out: 0.4635581649266757\n",
      "Mean actual_occup: 0.035495109111070636\n",
      "tensor([[0.0428, 0.0517],\n",
      "        [0.1601, 0.2010],\n",
      "        [0.0276, 0.0312],\n",
      "        ...,\n",
      "        [0.0293, 0.0376],\n",
      "        [0.6338, 0.7673],\n",
      "        [0.0159, 0.0178]], device='cuda:0')\n",
      "tensor(13.6599, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:23<00:00,  2.77it/s]\n",
      "  2%|▏         | 1/65 [00:00<00:10,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 0 - Avg stats (val_2)\n",
      "Mean loss: 27.806324746058536\n",
      "Mean out: 0.05001437974950442\n",
      "Mean occup: 0.0559863771383579\n",
      "Mean actual_out: 0.07174499582212705\n",
      "Mean actual_occup: 0.08150574467503108\n",
      "tensor([[0.0391, 0.0444],\n",
      "        [0.0440, 0.0489],\n",
      "        [0.0732, 0.0816],\n",
      "        ...,\n",
      "        [0.0526, 0.0556],\n",
      "        [0.0682, 0.0712],\n",
      "        [0.0117, 0.0115]], device='cuda:0')\n",
      "tensor(10.5872, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:37<00:00,  1.71it/s]\n",
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 0 - Avg stats (val_3)\n",
      "Mean loss: 108.8214602617117\n",
      "Mean out: 0.047176729515194894\n",
      "Mean occup: 0.08359355737383549\n",
      "Mean actual_out: 0.05243281760754494\n",
      "Mean actual_occup: 0.06198185212337054\n",
      "Flushed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:01<00:00,  4.06it/s]\n",
      "  2%|▏         | 1/65 [00:00<00:11,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 1 - Avg stats (train)\n",
      "Mean loss: 2.8365174160003663\n",
      "Mean out: 0.1445136812031269\n",
      "Mean occup: 0.05564787395298481\n",
      "Mean actual_out: 0.1458761837184429\n",
      "Mean actual_occup: 0.0559200499355793\n",
      "tensor([[0.1158, 0.1163],\n",
      "        [0.1198, 0.1196],\n",
      "        [0.1947, 0.1938],\n",
      "        ...,\n",
      "        [0.1467, 0.1463],\n",
      "        [0.1009, 0.0999],\n",
      "        [0.3305, 0.3315]], device='cuda:0')\n",
      "tensor(1.6990, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:13<00:00,  4.71it/s]\n",
      "  2%|▏         | 1/65 [00:00<00:07,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 1 - Avg stats (val_1)\n",
      "Mean loss: 9.11631864951207\n",
      "Mean out: 0.3966957954260019\n",
      "Mean occup: 0.0347370231953951\n",
      "Mean actual_out: 0.4635581649266757\n",
      "Mean actual_occup: 0.035495109111070636\n",
      "tensor([[0.0475, 0.0517],\n",
      "        [0.1945, 0.2010],\n",
      "        [0.0303, 0.0312],\n",
      "        ...,\n",
      "        [0.0323, 0.0376],\n",
      "        [0.7192, 0.7673],\n",
      "        [0.0173, 0.0178]], device='cuda:0')\n",
      "tensor(4.6380, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:24<00:00,  2.71it/s]\n",
      "  2%|▏         | 1/65 [00:00<00:10,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 1 - Avg stats (val_2)\n",
      "Mean loss: 16.689475932488076\n",
      "Mean out: 0.061506159775532206\n",
      "Mean occup: 0.06713325226536164\n",
      "Mean actual_out: 0.07174499582212705\n",
      "Mean actual_occup: 0.08150574467503108\n",
      "tensor([[0.0409, 0.0444],\n",
      "        [0.0457, 0.0489],\n",
      "        [0.0773, 0.0816],\n",
      "        ...,\n",
      "        [0.0544, 0.0556],\n",
      "        [0.0703, 0.0712],\n",
      "        [0.0119, 0.0115]], device='cuda:0')\n",
      "tensor(7.1059, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:38<00:00,  1.70it/s]\n",
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 1 - Avg stats (val_3)\n",
      "Mean loss: 20.984361740259025\n",
      "Mean out: 0.040700142773298115\n",
      "Mean occup: 0.047601951028292\n",
      "Mean actual_out: 0.05243281760754494\n",
      "Mean actual_occup: 0.06198185212337054\n",
      "Flushed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:01<00:00,  4.08it/s]\n",
      "  2%|▏         | 1/65 [00:00<00:12,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 2 - Avg stats (train)\n",
      "Mean loss: 1.7626609935760498\n",
      "Mean out: 0.14343630063533783\n",
      "Mean occup: 0.055033116534352305\n",
      "Mean actual_out: 0.1446768394112587\n",
      "Mean actual_occup: 0.05535947716236114\n",
      "tensor([[0.1159, 0.1163],\n",
      "        [0.1197, 0.1196],\n",
      "        [0.1940, 0.1938],\n",
      "        ...,\n",
      "        [0.1461, 0.1463],\n",
      "        [0.1011, 0.0999],\n",
      "        [0.3276, 0.3315]], device='cuda:0')\n",
      "tensor(1.6397, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:14<00:00,  4.56it/s]\n",
      "  2%|▏         | 1/65 [00:00<00:07,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 2 - Avg stats (val_1)\n",
      "Mean loss: 6.877020014249362\n",
      "Mean out: 0.4140479310200765\n",
      "Mean occup: 0.035571074170561934\n",
      "Mean actual_out: 0.4635581649266757\n",
      "Mean actual_occup: 0.035495109111070636\n",
      "tensor([[0.0481, 0.0517],\n",
      "        [0.1965, 0.2010],\n",
      "        [0.0306, 0.0312],\n",
      "        ...,\n",
      "        [0.0335, 0.0376],\n",
      "        [0.7228, 0.7673],\n",
      "        [0.0177, 0.0178]], device='cuda:0')\n",
      "tensor(3.7119, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:24<00:00,  2.70it/s]\n",
      "  2%|▏         | 1/65 [00:00<00:11,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 2 - Avg stats (val_2)\n",
      "Mean loss: 14.936544099220862\n",
      "Mean out: 0.06267550604847762\n",
      "Mean occup: 0.0686990190583926\n",
      "Mean actual_out: 0.07174499582212705\n",
      "Mean actual_occup: 0.08150574467503108\n",
      "tensor([[0.0414, 0.0444],\n",
      "        [0.0462, 0.0489],\n",
      "        [0.0781, 0.0816],\n",
      "        ...,\n",
      "        [0.0546, 0.0556],\n",
      "        [0.0700, 0.0712],\n",
      "        [0.0118, 0.0115]], device='cuda:0')\n",
      "tensor(6.4084, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:38<00:00,  1.69it/s]\n",
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 2 - Avg stats (val_3)\n",
      "Mean loss: 21.27042975242321\n",
      "Mean out: 0.04133995599471606\n",
      "Mean occup: 0.0477859344619971\n",
      "Mean actual_out: 0.05243281760754494\n",
      "Mean actual_occup: 0.06198185212337054\n",
      "Flushed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:12<00:00,  3.43it/s]\n",
      "  2%|▏         | 1/65 [00:00<00:11,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 3 - Avg stats (train)\n",
      "Mean loss: 1.5055668873786927\n",
      "Mean out: 0.14388746348023415\n",
      "Mean occup: 0.055380867511034014\n",
      "Mean actual_out: 0.14478018295764922\n",
      "Mean actual_occup: 0.05561791653931141\n",
      "tensor([[0.1132, 0.1163],\n",
      "        [0.1168, 0.1196],\n",
      "        [0.1896, 0.1938],\n",
      "        ...,\n",
      "        [0.1431, 0.1463],\n",
      "        [0.0985, 0.0999],\n",
      "        [0.3214, 0.3315]], device='cuda:0')\n",
      "tensor(3.0263, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:14<00:00,  4.56it/s]\n",
      "  2%|▏         | 1/65 [00:00<00:07,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 3 - Avg stats (val_1)\n",
      "Mean loss: 7.621907160832332\n",
      "Mean out: 0.41535829626596893\n",
      "Mean occup: 0.03521216454414221\n",
      "Mean actual_out: 0.4635581649266757\n",
      "Mean actual_occup: 0.035495109111070636\n",
      "tensor([[0.0476, 0.0517],\n",
      "        [0.1967, 0.2010],\n",
      "        [0.0301, 0.0312],\n",
      "        ...,\n",
      "        [0.0336, 0.0376],\n",
      "        [0.7209, 0.7673],\n",
      "        [0.0175, 0.0178]], device='cuda:0')\n",
      "tensor(3.9450, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:23<00:00,  2.72it/s]\n",
      "  2%|▏         | 1/65 [00:00<00:11,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Epoch 3 - Avg stats (val_2)\n",
      "Mean loss: 15.109490702702448\n",
      "Mean out: 0.0626468264999298\n",
      "Mean occup: 0.06867184409728418\n",
      "Mean actual_out: 0.07174499582212705\n",
      "Mean actual_occup: 0.08150574467503108\n",
      "tensor([[0.0407, 0.0444],\n",
      "        [0.0454, 0.0489],\n",
      "        [0.0768, 0.0816],\n",
      "        ...,\n",
      "        [0.0534, 0.0556],\n",
      "        [0.0684, 0.0712],\n",
      "        [0.0115, 0.0115]], device='cuda:0')\n",
      "tensor(7.7631, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 37/65 [00:11<00:08,  3.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f7648db9743a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m                         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                     out, occup = model(sample,means,stds,mode,baseline_occup=b_occup,\n\u001b[0m\u001b[1;32m     70\u001b[0m                                       baseline_out=b_out)\n\u001b[1;32m     71\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gcn2/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-23ab90e48957>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, means, stds, naive, mode, baseline_occup, baseline_out)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_numpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m                 \u001b[0mH\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_pl_r_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_hidden\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmax_numpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gcn2/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gcn2/lib/python3.9/site-packages/torch_geometric_temporal/nn/recurrent/gconv_gru.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, edge_index, edge_weight, H)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \"\"\"\n\u001b[1;32m    148\u001b[0m         \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_hidden_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_update_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_reset_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mH_tilde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_candidate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gcn2/lib/python3.9/site-packages/torch_geometric_temporal/nn/recurrent/gconv_gru.py\u001b[0m in \u001b[0;36m_calculate_update_gate\u001b[0;34m(self, X, edge_index, edge_weight, H)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_update_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_x_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_h_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gcn2/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gcn2/lib/python3.9/site-packages/torch_geometric/nn/conv/cheb_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight, batch, lambda_max)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlambda_max\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         edge_index, norm = self.__norm__(edge_index, x.size(self.node_dim),\n\u001b[0m\u001b[1;32m    127\u001b[0m                                          \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                                          \u001b[0mlambda_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gcn2/lib/python3.9/site-packages/torch_geometric/nn/conv/cheb_conv.py\u001b[0m in \u001b[0;36m__norm__\u001b[0;34m(self, edge_index, num_nodes, edge_weight, normalization, lambda_max, dtype, batch)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_self_loops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         edge_index, edge_weight = get_laplacian(edge_index, edge_weight,\n\u001b[0m\u001b[1;32m     96\u001b[0m                                                 \u001b[0mnormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                                                 num_nodes)\n",
      "\u001b[0;32m~/anaconda3/envs/gcn2/lib/python3.9/site-packages/torch_geometric/utils/get_laplacian.py\u001b[0m in \u001b[0;36mget_laplacian\u001b[0;34m(edge_index, edge_weight, normalization, dtype, num_nodes)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mdeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gcn2/lib/python3.9/site-packages/torch_scatter/scatter.py\u001b[0m in \u001b[0;36mscatter_add\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 dim_size: Optional[int] = None) -> torch.Tensor:\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mscatter_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gcn2/lib/python3.9/site-packages/torch_scatter/scatter.py\u001b[0m in \u001b[0;36mscatter_sum\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 dim_size: Optional[int] = None) -> torch.Tensor:\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gcn2/lib/python3.9/site-packages/torch_scatter/utils.py\u001b[0m in \u001b[0;36mbroadcast\u001b[0;34m(src, other, dim)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm,trange\n",
    "from convertDataset import total_samples\n",
    "from torch.nn import MSELoss\n",
    "def MAPE(preds,actuals):\n",
    "    return 100.0*torch.mean(torch.abs((preds-actuals)/actuals))\n",
    "\n",
    "def mape_all(preds,actuals):\n",
    "    return 100.0*torch.abs((preds-actuals)/actuals)\n",
    "def lMAPE(preds,actuals):\n",
    "    return 100.0*torch.mean(torch.abs((torch.log(preds)-torch.log(actuals))/actuals))\n",
    "\n",
    "def MSE(preds,actuals):\n",
    "    return torch.sqrt(torch.mean(torch.square(preds-actuals)))\n",
    "import numpy as np\n",
    "\n",
    "model = ChallengeModel().cuda()\n",
    "baseline = Baseline().cuda()\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "num_epochs = 300\n",
    "opt = torch.optim.Adam(lr=1e-3,params=model.parameters())\n",
    "step = 0\n",
    "\n",
    "#model.load_state_dict('./model/model_3650.pt')\n",
    "torch.manual_seed(420)\n",
    "        \n",
    "\n",
    "#model.load_state_dict(torch.load(f'./18_setembro_modelo_2.pt'))\n",
    "#model.load_state_dict(torch.load(f'./model_18set/model_{63}.pt'))\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "        \n",
    "    for mode in ['train','val_1','val_2','val_3']:\n",
    "        if mode == 'train':\n",
    "                model.train()\n",
    "        else:\n",
    "                model.eval()\n",
    "        stats = {'loss':[],\n",
    "                 'out':[],\n",
    "                 'occup':[],\n",
    "                 'actual_out':[],\n",
    "                 'actual_occup':[]\n",
    "                }\n",
    "                \n",
    "        running_loss = []\n",
    "        out_mean = []\n",
    "        occup_mean = []\n",
    "        actual_out_mean = []\n",
    "        cnt = 0\n",
    "        total=len(dataloaders[mode])//30 if mode == 'train' else len(dataloaders[mode])\n",
    "        mode_maybe_shuffle = 'train_s' if mode == 'train' else mode\n",
    "        for i,sample in tqdm(enumerate(dataloaders[mode_maybe_shuffle]),\n",
    "                             total=total):\n",
    "            \n",
    "            #with torch.autograd.detect_anomaly():\n",
    "\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    b_out, b_occup = baseline(sample,means,stds,mode)\n",
    "\n",
    "\n",
    "                with torch.set_grad_enabled(mode == 'train'):\n",
    "                    if i == total:\n",
    "                        break\n",
    "                    if mode == 'train':\n",
    "                        opt.zero_grad()\n",
    "                    cnt += 1\n",
    "                    out, occup = model(sample,means,stds,mode,baseline_occup=b_occup,\n",
    "                                      baseline_out=b_out)\n",
    "                    if False:\n",
    "                        loss = mape_all(out,sample.out_delay)\n",
    "                        loss = scatter(src=loss,index=sample.batch[sample.type==0],\n",
    "                                       dim=0,dim_size=(sample.batch.max()+1),reduce='mean')\n",
    "                        loss = loss.mean()\n",
    "                    else:\n",
    "                        loss = MAPE(out,sample.out_delay)\n",
    "                    if mode == 'train':\n",
    "                        #MSE(occup,sample.out_occupancy).backward()\n",
    "                        lMAPE(out,sample.out_delay).backward()\n",
    "                        opt.step()\n",
    "                    elif i == 0:\n",
    "                        print(torch.cat([out.view(-1,1),sample.out_delay.view(-1,1)],axis=1))\n",
    "                        print(loss)\n",
    "\n",
    "                    _stats = {'loss':loss,\n",
    "                              'out':out.mean(),\n",
    "                              'actual_out':sample.out_delay.mean() if not mode == 'test' else -1.0,\n",
    "                              'occup':occup.mean(),\n",
    "                              'actual_occup':sample.out_occupancy.mean() if not mode == 'test' else -1.0,\n",
    "                             }\n",
    "                    for k in _stats.keys():\n",
    "                        stats[k].append(_stats[k].cpu().item())\n",
    "                    del _stats\n",
    "                if i == -1:\n",
    "\n",
    "                    batch = sample.batch[sample.type==0]\n",
    "                    for b in range(sample.batch.max() + 1):\n",
    "                        out_batch = out[batch==b].clone().detach().cpu().view(-1).numpy().round(5)  \n",
    "                        out_target =  sample.out_delay[batch==b].cpu().view(-1).numpy().round(5)  \n",
    "                        import matplotlib.pyplot as plt\n",
    "                        plt.hist(out_target,density=True,bins=100,\n",
    "                                 color='green',alpha=0.5)\n",
    "\n",
    "                        plt.hist(out_batch,density=True,bins=10)\n",
    "                        plt.show()\n",
    "\n",
    "                        cm = plt.cm.get_cmap('RdYlBu_r')\n",
    "                        sc = plt.scatter(out_target,\n",
    "                                    out_batch,\n",
    "                                   c=mape_all(torch.FloatTensor(out_target),\n",
    "                                              torch.FloatTensor(out_batch)).clone().detach().view(-1).cpu().numpy(),\n",
    "                                   cmap=cm)\n",
    "                        plt.colorbar(sc)\n",
    "                        plt.show()\n",
    "\n",
    "        print(\"======================================\")\n",
    "        print(f\"Epoch {epoch} - Avg stats ({mode})\")\n",
    "        for k in stats.keys():\n",
    "            print(f'Mean {k}: {np.array(stats[k]).mean()}')\n",
    "            writer.add_scalar(f\"{k}/{mode}\", np.array(stats[k]).mean(), step)\n",
    "    writer.flush()\n",
    "    print(\"Flushed\")\n",
    "    step += 1\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    import os\n",
    "    os.makedirs('./model',exist_ok=True)\n",
    "    if i%1 == 0:\n",
    "        torch.save(model.state_dict(),f'./model/model_{epoch}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83d7d3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load(f'./model_18set/model_{63}.pt'))\n",
    "model.load_state_dict(torch.load(f'./22_setembro_modelo.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de317e",
   "metadata": {},
   "source": [
    "## Create Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97dcc33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [06:09<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "import os\n",
    "os.makedirs('./predictions',exist_ok=True)\n",
    "FILENAME = '27_setembro_1.csv'\n",
    "upload_file = open(f'./predictions/{FILENAME}', \"w\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "cnt = 0\n",
    "for i,sample in tqdm(enumerate(dataloaders['test']),total=len(dataloaders['test'])):\n",
    "    with torch.set_grad_enabled(False):\n",
    "        b_out, b_occup = baseline(sample,means,stds,mode)\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        out, occup = model(sample,means,stds,'test',baseline_occup=b_occup,\n",
    "                                      baseline_out=b_out)\n",
    "        batch = sample.batch[sample.type==0]\n",
    "\n",
    "        for b in range(sample.batch.max() + 1):\n",
    "            if cnt > 0:\n",
    "                upload_file.write(\"\\n\")\n",
    "            cnt += 1\n",
    "            out_batch = out[batch==b].cpu().numpy().round(5)            \n",
    "            upload_file.write(\"{}\".format(';'.join([str(i) for i in np.squeeze(out_batch)])))\n",
    "        \n",
    "upload_file.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d258df9",
   "metadata": {},
   "source": [
    "### Check if test file is OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "3c258536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the file...\n",
      "Congratulations! The submission file has passed all the tests!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from itertools import zip_longest\n",
    "def check_submission(FILENAME,PATHS_PER_SAMPLE):\n",
    "    sample_num = 0\n",
    "    error = False\n",
    "    with open(FILENAME, \"r\") as uploaded_file, open(PATHS_PER_SAMPLE, \"r\") as path_per_sample:\n",
    "        # Load all files line by line (not at once)\n",
    "        for prediction, n_paths in zip_longest(uploaded_file, path_per_sample):\n",
    "            # Case 1: Line Count does not match.\n",
    "            if n_paths is None:\n",
    "                print(\"WARNING: File must contain 1560 lines in total for the final test datset (90 for the toy dataset). \"\n",
    "                      \"Looks like the uploaded file has {} lines\".format(sample_num))\n",
    "                error = True\n",
    "            if prediction is None:\n",
    "                print(\"WARNING: File must have 1560 lines in total for the final test datset (90 for the toy dataset). \"\n",
    "                      \"Looks like the uploaded file has {} lines\".format(sample_num))\n",
    "                error = True\n",
    "\n",
    "            # Remove the \\n at the end of lines\n",
    "            prediction = prediction.rstrip()\n",
    "            n_paths = n_paths.rstrip()\n",
    "\n",
    "            # Split the line, convert to float and then, to list\n",
    "            prediction = list(map(float, prediction.split(\";\")))\n",
    "\n",
    "            # Case 2: Wrong number of predictions in a sample\n",
    "            if int(len(prediction)) != int(n_paths):\n",
    "                print(\"WARNING in line {}: The line should have size {} but it has size {}\".format(sample_num, n_paths,\n",
    "                                                                                                   len(prediction)))\n",
    "                error = True\n",
    "\n",
    "            sample_num += 1\n",
    "\n",
    "    if not error:\n",
    "        print(\"Congratulations! The submission file has passed all the tests!\")\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "        \n",
    "print(\"Checking the file...\")\n",
    "\n",
    "PATHS_PER_SAMPLE = './paths_per_sample_test_dataset.txt'\n",
    "FILEPATH= f'./predictions/{FILENAME}'\n",
    "check_submission(FILEPATH,PATHS_PER_SAMPLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664224be",
   "metadata": {},
   "source": [
    "## Create Gephi graphs for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a8d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import gc\n",
    "import networkx as nx\n",
    "stats.clear()\n",
    "gc.collect()\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import subgraph, to_networkx\n",
    "\n",
    "os.makedirs('./gephi',exist_ok=True)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for mode in ['train','val_1','val_2','val_3']:\n",
    "        model.eval()\n",
    "        for data in datasets[mode]:\n",
    "            \n",
    "            data = data.clone()#Copy data object\n",
    "            G,P,L = data.G,data.P,data.L\n",
    "            is_p = data.type==0\n",
    "            is_l = data.type==1\n",
    "            is_n = data.type==2       \n",
    "            \n",
    "            G_dim, L_dim, P_dim = G.shape[1],L.shape[1],P.shape[1]\n",
    "            \n",
    "            X = torch.zeros(data.G.size(0),G_dim+L_dim+P_dim+4,device='cuda')\n",
    "            cnt = 0\n",
    "            cnt, node_og     = cnt+G_dim, slice(cnt,cnt+G_dim)\n",
    "            cnt, link_og     = cnt+L_dim, slice(cnt,cnt+L_dim)\n",
    "            cnt, path_og     = cnt+P_dim, slice(cnt,cnt+P_dim)\n",
    "\n",
    "            X[:,node_og] = G[:,:]\n",
    "            \n",
    "            X[is_l,link_og] = L \n",
    "            X[is_p,link_og] = L.mean(axis=0)\n",
    "            X[is_n,link_og] = L.mean(axis=0)\n",
    "            \n",
    "            X[is_p,path_og] = P\n",
    "            X[is_l,path_og] = P.mean(axis=0)\n",
    "            X[is_n,path_og] = P.mean(axis=0)\n",
    "            \n",
    "            \n",
    "            with torch.set_grad_enabled(False):\n",
    "                out, occup = model(data,means,stds)\n",
    "                X[is_p,-4] = out.view(-1)\n",
    "                X[is_l,-3] = occup.view(-1)\n",
    "                \n",
    "            with torch.set_grad_enabled(False):\n",
    "                X[is_p,-2] = data.out_delay.view(-1)\n",
    "                X[is_l,-1] = data.out_occupancy.view(-1)\n",
    "            \n",
    "            data.PRED = X[:,(-4):(-2)]\n",
    "            data.ACTUAL = X[:,-2:]\n",
    "            \n",
    "            node_attrs = ['type']\n",
    "            cnt = 0\n",
    "            for x in ['G','L','P','PRED','ACTUAL']:\n",
    "                for j in range(getattr(data,x).shape[1]):\n",
    "                    node_attrs.append(f'{x}_{j}')\n",
    "                    setattr(data,f'{x}_{j}',X[:,cnt].view(-1,1))\n",
    "                    cnt += 1\n",
    "                delattr(data,x)\n",
    "            \n",
    "            \n",
    "            edge_attrs = []\n",
    "            print(data)\n",
    "            g = to_networkx(data,node_attrs=node_attrs,edge_attrs=edge_attrs)\n",
    "            nx.write_gexf(g, f\"./gephi/{mode}_{i}.gexf\")\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9767480",
   "metadata": {},
   "source": [
    "## Plot some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea9b5891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/750 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1190, 5])\n",
      "torch.Size([344, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([89700, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4160, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dct = {'G':[],\n",
    "       'P':[],\n",
    "       'L':[],\n",
    "       'true_occup':[],\n",
    "       'true_delay':[],\n",
    "       'pred_occup':[],\n",
    "       'pred_delay':[],\n",
    "       'mode_p':[],\n",
    "       'mode_g':[],\n",
    "       'mode_l':[],\n",
    "      \n",
    "      }\n",
    "\n",
    "for m,mode in enumerate(['train','val_1','val_2','val_3']):\n",
    "        model.eval()\n",
    "    \n",
    "        cnt = 0\n",
    "        total=len(dataloaders[mode])//10 if mode == 'train' else len(dataloaders[mode])\n",
    "        mode_maybe_shuffle = 'train_s' if mode == 'train' else mode\n",
    "        for i,data in tqdm(enumerate(datasets[mode]),\n",
    "                             total=total):\n",
    "            \n",
    "            data = data.clone()#Copy data object\n",
    "            G,P,L = data.G,data.P,data.L\n",
    "            print(P.shape)\n",
    "            def _np(x):\n",
    "                return x.clone().detach().cpu().numpy()\n",
    "            \n",
    "            for k in ['G','P','L']:\n",
    "                dct[k].append(_np(getattr(data,k)))\n",
    "            with torch.set_grad_enabled(False):\n",
    "                out, occup = model(data,means,stds)\n",
    "            dct['pred_delay'].append(_np(out.view(-1,1) ))\n",
    "            dct['pred_occup'].append(_np(occup.view(-1,1)))\n",
    "            \n",
    "            dct['true_delay'].append(_np(data.out_delay.view(-1,1)))\n",
    "            dct['true_occup'].append(_np(data.out_occupancy.view(-1,1)))\n",
    "            \n",
    "            dct['mode_p'].append(_np(torch.full((P.shape[0],),m).view(-1,1)))\n",
    "            dct['mode_g'].append(_np(torch.full((G.shape[0],),m).view(-1,1)))\n",
    "            dct['mode_l'].append(_np(torch.full((L.shape[0],),m).view(-1,1)))\n",
    "\n",
    "            break\n",
    "            \n",
    "            \n",
    "for k in dct.keys():\n",
    "    dct[k] = np.concatenate(dct[k],axis=0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0054ec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g_delay</th>\n",
       "      <th>g_packets</th>\n",
       "      <th>g_losses</th>\n",
       "      <th>g_AvgPktsLambda</th>\n",
       "      <th>p_time_EqLambda</th>\n",
       "      <th>p_time_AvgPktsLambda</th>\n",
       "      <th>p_TotalPktsGen</th>\n",
       "      <th>p_PktsGen</th>\n",
       "      <th>p_AvgBw</th>\n",
       "      <th>link_capacity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.284</td>\n",
       "      <td>1197.469971</td>\n",
       "      <td>9.203</td>\n",
       "      <td>1835.949951</td>\n",
       "      <td>1006.278015</td>\n",
       "      <td>1.006</td>\n",
       "      <td>33282.554688</td>\n",
       "      <td>1.006</td>\n",
       "      <td>1006.265015</td>\n",
       "      <td>39437.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.625</td>\n",
       "      <td>306.639008</td>\n",
       "      <td>1.667</td>\n",
       "      <td>1612.262939</td>\n",
       "      <td>891.333008</td>\n",
       "      <td>0.891</td>\n",
       "      <td>32424.337891</td>\n",
       "      <td>0.891</td>\n",
       "      <td>891.630005</td>\n",
       "      <td>42307.691406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012</td>\n",
       "      <td>38604.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>781.413025</td>\n",
       "      <td>430.329987</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2194.875</td>\n",
       "      <td>0.43</td>\n",
       "      <td>430.367004</td>\n",
       "      <td>544606.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.051</td>\n",
       "      <td>1568.180054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>692.468994</td>\n",
       "      <td>376.985992</td>\n",
       "      <td>0.377</td>\n",
       "      <td>2167.556885</td>\n",
       "      <td>0.377</td>\n",
       "      <td>377.088989</td>\n",
       "      <td>150675.65625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  g_delay    g_packets g_losses g_AvgPktsLambda p_time_EqLambda  \\\n",
       "0   0.284  1197.469971    9.203     1835.949951     1006.278015   \n",
       "1   0.625   306.639008    1.667     1612.262939      891.333008   \n",
       "2   0.012      38604.0      0.0      781.413025      430.329987   \n",
       "3   0.051  1568.180054      0.0      692.468994      376.985992   \n",
       "\n",
       "  p_time_AvgPktsLambda p_TotalPktsGen p_PktsGen      p_AvgBw link_capacity  \n",
       "0                1.006   33282.554688     1.006  1006.265015       39437.5  \n",
       "1                0.891   32424.337891     0.891   891.630005  42307.691406  \n",
       "2                 0.43       2194.875      0.43   430.367004    544606.125  \n",
       "3                0.377    2167.556885     0.377   377.088989  150675.65625  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "pallette = cm.get_cmap('Pastel1')\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "timeparams = [f'p_time_{a}' for a in ['EqLambda', 'AvgPktsLambda']]\n",
    "p_params = timeparams + ['p_TotalPktsGen','p_PktsGen','p_AvgBw']\n",
    "g_params = ['g_delay','g_packets','g_losses','g_AvgPktsLambda']\n",
    "l_params = ['link_capacity']\n",
    "names = {'G': g_params,\n",
    "         'P':p_params,\n",
    "         'L':l_params\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(columns = g_params+p_params+l_params,index=range(4))\n",
    "print()\n",
    "for m in range(4):\n",
    "    for t in ['G','P','L']:\n",
    "        for i,name in enumerate(names[t]):\n",
    "            val = dct[t][:,i][dct[f'mode_{t.lower()}'].reshape(-1)==m].mean(axis=0).round(3)\n",
    "            getattr(df,name).values[m] = val\n",
    "    \n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202aa6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
